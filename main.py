import asyncio
import aiohttp
import logging
import os
from collections import OrderedDict
import re
import time
import shutil
import argparse  # æ·»åŠ argparseåº“æ¥è§£æå‘½ä»¤è¡Œå‚æ•°

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# åœ¨æ–‡ä»¶å¼€å¤´ä¿®æ”¹é…ç½®
# EPG_URL = "http://epg.51zmt.top:8000/e.xml"  # EPG æº
# LOGO_URL = "http://epg.51zmt.top:8000/pics"  # ä¿®æ”¹å°æ ‡åŸºç¡€URL

# è¯»å–è®¢é˜…æ–‡ä»¶ä¸­çš„ URL
def read_subscribe_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        logging.error(f"æœªæ‰¾åˆ°è®¢é˜…æ–‡ä»¶: {file_path}")
        return []


# è¯»å–åŒ…å«æƒ³ä¿ç•™çš„ç»„åæˆ–é¢‘é“çš„æ–‡ä»¶
def read_include_list_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        logging.error(f"æœªæ‰¾åˆ°åŒ…å«åˆ—è¡¨æ–‡ä»¶: {file_path}")
        return []


# å¼‚æ­¥è·å– URL å†…å®¹å¹¶æµ‹è¯•å“åº”æ—¶é—´
async def fetch_url(session, url):
    start_time = time.time()
    try:
        async with session.get(url, timeout=10) as response:
            if response.status == 200:
                content = await response.text()
                elapsed_time = time.time() - start_time
                return content, elapsed_time
            else:
                logging.warning(f"è¯·æ±‚ {url} å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
    except Exception as e:
        logging.error(f"è¯·æ±‚ {url} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    return None, float('inf')


# è§£æ M3U æ ¼å¼å†…å®¹
def parse_m3u_content(content):
    channels = []
    lines = content.splitlines()
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if line.startswith('#EXTINF:'):
            info = line.split(',', 1)
            if len(info) == 2:
                metadata = info[0]
                name = info[1]
                tvg_id = re.search(r'tvg-id="([^"]+)"', metadata)
                tvg_name = re.search(r'tvg-name="([^"]+)"', metadata)
                tvg_logo = re.search(r'tvg-logo="([^"]+)"', metadata)
                group_title = re.search(r'group-title="([^"]+)"', metadata)
                i += 1
                if i < len(lines):
                    url = lines[i].strip()
                    channel = {
                        'name': name,
                        'url': url,
                        'tvg_id': tvg_id.group(1) if tvg_id else None,
                        'tvg_name': tvg_name.group(1) if tvg_name else None,
                        'tvg_logo': tvg_logo.group(1) if tvg_logo else None,
                        'group_title': group_title.group(1) if group_title else None,
                        'response_time': float('inf')
                    }
                    channels.append(channel)
        i += 1
    return channels


# è§£æ TXT æ ¼å¼å†…å®¹
def parse_txt_content(content):
    channels = []
    current_group = None
    lines = content.splitlines()
    for line in lines:
        line = line.strip()
        if line.endswith('#genre#'):
            current_group = line.replace('#genre#', '').strip()
        elif line:
            parts = line.split(',', 1)
            if len(parts) == 2:
                name, url = parts
                channel = {
                    'name': name,
                    'url': url,
                    'tvg_id': None,
                    'tvg_name': None,
                    'group_title': current_group,
                    'response_time': float('inf')
                }
                channels.append(channel)
    return channels


# åˆå¹¶å¹¶å»é‡é¢‘é“
def merge_and_deduplicate(channels_list):
    all_channels = []
    for channels in channels_list:
        all_channels.extend(channels)
    unique_channels = []
    url_set = set()
    for channel in all_channels:
        if channel['url'] not in url_set:
            unique_channels.append(channel)
            url_set.add(channel['url'])
    return unique_channels


# æµ‹è¯•æ¯ä¸ªé¢‘é“çš„å“åº”æ—¶é—´
async def test_channel_response_time(session, channel):
    start_time = time.time()
    try:
        async with session.get(channel['url'], timeout=10) as response:
            if response.status == 200:
                elapsed_time = time.time() - start_time
                channel['response_time'] = elapsed_time
    except Exception as e:
        logging.error(f"æµ‹è¯• {channel['url']} å“åº”æ—¶é—´æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    return channel


# åˆ†ç»„æ˜ å°„å…³ç³»
GROUP_MAPPING = {
    'å¤®è§†é¢‘é“': 'ğŸ“å¤®è§†é¢‘é“',
    'CCTV': 'ğŸ“å¤®è§†é¢‘é“',
    'å«è§†é¢‘é“': 'ğŸ§å«è§†é¢‘é“',
    'å«è§†': 'ğŸ§å«è§†é¢‘é“',
    'æ¹–å—': 'ğŸ„æ¹–å—é¢‘é“',
    'æ¸¯æ¾³å°': 'ğŸ¦„ï¸æ¸¯Â·æ¾³Â·å°',
    'æ¸¯Â·æ¾³Â·å°': 'ğŸ¦„ï¸æ¸¯Â·æ¾³Â·å°',
    'æ–¯ç›ç‰¹': None,  # None è¡¨ç¤ºè¦æ’é™¤çš„åˆ†ç»„
    'æ–¯ç›ç‰¹,': None
}

def normalize_group_title(title):
    """æ ‡å‡†åŒ–åˆ†ç»„æ ‡é¢˜"""
    if not title:
        return ''
    
    # ç§»é™¤æœ«å°¾çš„ #genre# å’Œé€—å·
    title = title.strip().rstrip('#genre#').rstrip(',').strip()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜ å°„åˆ°å…¶ä»–åˆ†ç»„
    for old_group, new_group in GROUP_MAPPING.items():
        if old_group in title:
            return new_group if new_group else ''
            
    return title

def normalize_channel_name(name):
    """æ ‡å‡†åŒ–é¢‘é“åç§°"""
    name = name.strip().upper()
    variants = set()  # ä½¿ç”¨é›†åˆå»é‡
    
    # å¤„ç† CCTV-1/CCTV1 è¿™æ ·çš„è§„åˆ™
    if '/' in name:
        parts = [n.strip() for n in name.split('/')]
    else:
        parts = [name]
        
    for part in parts:
        # æ·»åŠ åŸå§‹æ ¼å¼
        variants.add(part)
        
        # å¦‚æœåŒ…å« CCTVï¼Œç”Ÿæˆä¸åŒçš„å˜ä½“
        if 'CCTV' in part:
            # ç§»é™¤æ‰€æœ‰åˆ†éš”ç¬¦
            clean_name = part.replace('-', '').replace('_', '').replace(' ', '')
            variants.add(clean_name)
            
            # æå–æ•°å­—éƒ¨åˆ†
            number = ''.join(c for c in clean_name if c.isdigit())
            if number:
                # æ·»åŠ å¸¦è¿å­—ç¬¦çš„ç‰ˆæœ¬
                variants.add(f'CCTV-{number}')
                # æ·»åŠ ä¸å¸¦è¿å­—ç¬¦çš„ç‰ˆæœ¬
                variants.add(f'CCTV{number}')
                
    return list(variants)

def get_channel_id(name):
    """æ ¹æ®é¢‘é“åç§°è·å–å¯¹åº”çš„é¢‘é“ID"""
    # å¸¸è§é¢‘é“IDæ˜ å°„
    channel_ids = {
        'CCTV-1': 'cctv1',
        'CCTV1': 'cctv1',
        'CCTV-2': 'cctv2',
        'CCTV2': 'cctv2',
        'CCTV-3': 'cctv3',
        'CCTV3': 'cctv3',
        'CCTV-4': 'cctv4',
        'CCTV4': 'cctv4',
        'CCTV-5': 'cctv5',
        'CCTV5': 'cctv5',
        'CCTV-5+': 'cctv5plus',
        'CCTV5+': 'cctv5plus',
        'CCTV-6': 'cctv6',
        'CCTV6': 'cctv6',
        'CCTV-7': 'cctv7',
        'CCTV7': 'cctv7',
        'CCTV-8': 'cctv8',
        'CCTV8': 'cctv8',
        'CCTV-9': 'cctv9',
        'CCTV9': 'cctv9',
        'CCTV-10': 'cctv10',
        'CCTV10': 'cctv10',
        'CCTV-11': 'cctv11',
        'CCTV11': 'cctv11',
        'CCTV-12': 'cctv12',
        'CCTV12': 'cctv12',
        'CCTV-13': 'cctv13',
        'CCTV13': 'cctv13',
        'CCTV-14': 'cctv14',
        'CCTV14': 'cctv14',
        'CCTV-15': 'cctv15',
        'CCTV15': 'cctv15',
        'CCTV-16': 'cctv16',
        'CCTV16': 'cctv16',
        'CCTV-17': 'cctv17',
        'CCTV17': 'cctv17',
        'æ¹–å—å«è§†': 'hunan',
        'æµ™æ±Ÿå«è§†': 'zhejiang',
        'æ±Ÿè‹å«è§†': 'jiangsu',
        'åŒ—äº¬å«è§†': 'beijing',
        'ä¸œæ–¹å«è§†': 'dongfang',
        'å®‰å¾½å«è§†': 'anhui',
        'å¹¿ä¸œå«è§†': 'guangdong',
        'æ·±åœ³å«è§†': 'shenzhen',
        'è¾½å®å«è§†': 'liaoning',
        'å±±ä¸œå«è§†': 'shandong',
        'é»‘é¾™æ±Ÿå«è§†': 'heilongjiang',
        'æ¹–åŒ—å«è§†': 'hubei',
        'æ²³å—å«è§†': 'henan',
        'é™•è¥¿å«è§†': 'shanxi',
        'å››å·å«è§†': 'sichuan',
        'é‡åº†å«è§†': 'chongqing',
        'æ±Ÿè¥¿å«è§†': 'jiangxi',
        'è´µå·å«è§†': 'guizhou',
        'æ²³åŒ—å«è§†': 'hebei',
        'ç¦å»ºå«è§†': 'fujian',
        'ä¸œå—å«è§†': 'dongnan',
        'æµ·å—å«è§†': 'hainan',
        'äº‘å—å«è§†': 'yunnan',
        'å‰æ—å«è§†': 'jilin',
        'å†…è’™å¤å«è§†': 'neimeng',
        'ç”˜è‚ƒå«è§†': 'gansu',
        'å®å¤å«è§†': 'ningxia',
        'é’æµ·å«è§†': 'qinghai',
        'è¥¿è—å«è§†': 'xizang',
        'æ–°ç–†å«è§†': 'xinjiang',
        'å‡¤å‡°ä¸­æ–‡': 'fenghuangzhongwen',
        'å‡¤å‡°èµ„è®¯': 'fenghuangzixun',
        'ç¿¡ç¿ å°': 'tvb',
        'TVBç¿¡ç¿ å°': 'tvb',
        'æ˜ç å°': 'pearl',
        'Pearlæ˜ç å°': 'pearl',
        'TVBSæ–°é—»': 'tvbs',
        'æ— çº¿æ–°é—»': 'tvbnews',
        'æ¹–å—éƒ½å¸‚': 'hunandushi',
        'æ¹–å—ç»è§†': 'hunanjingshi',
        'æ¹–å—å¨±ä¹': 'hunanyule',
        'é‡‘é¹°çºªå®': 'jinyingjishi',
        'å¿«ä¹å‚é’“': 'kuailechuidiao'
    }
    
    # æ¸…ç†é¢‘é“åç§°
    name = name.upper().strip()
    
    # å°è¯•ç›´æ¥åŒ¹é…
    if name in channel_ids:
        return channel_ids[name]
    
    # å¤„ç†CCTVé¢‘é“
    if 'CCTV' in name:
        number = ''.join(filter(str.isdigit, name))
        if number:
            return f'cctv{number}'
    
    # å¤„ç†å«è§†é¢‘é“
    if 'å«è§†' in name:
        province = name.replace('å«è§†', '').strip()
        pinyin = {
            'åŒ—äº¬': 'beijing',
            'ä¸œæ–¹': 'dongfang',
            'æµ™æ±Ÿ': 'zhejiang',
            'æ±Ÿè‹': 'jiangsu',
            'æ¹–å—': 'hunan',
            'å®‰å¾½': 'anhui',
            # ... å¯ä»¥æ·»åŠ æ›´å¤šæ‹¼éŸ³æ˜ å°„
        }
        if province in pinyin:
            return pinyin[province]
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œè¿”å›å°å†™çš„é¢‘é“å
    return name.lower()

def filter_channels(channels, include_list):
    filtered_channels = []
    current_group = None
    channel_group_mapping = {}
    channel_name_mapping = {}  # å­˜å‚¨æ ‡å‡†é¢‘é“åæ˜ å°„
    processed_channels = set()  # ç”¨äºå»é‡
    channel_variants = {}  # å­˜å‚¨é¢‘é“åç§°çš„ä¸åŒå˜ä½“
    allowed_channels = set()  # å­˜å‚¨å…è®¸çš„é¢‘é“åç§°å˜ä½“
    
    # è§£æ include_list ä¸­çš„åˆ†ç»„ä¿¡æ¯å’Œé¢‘é“åç§°
    for line in include_list:
        line = line.strip()
        if line.startswith('group:'):
            current_group = line.replace('group:', '').strip()
        elif line and current_group:
            # è·å–é¢‘é“åç§°çš„æ‰€æœ‰å˜ä½“
            original_name = line  # ä¿å­˜åŸå§‹åç§°ä½œä¸ºæ ‡å‡†åç§°
            variants = normalize_channel_name(line)
            for variant in variants:
                channel_variants[variant] = current_group
                channel_name_mapping[variant] = original_name  # è®°å½•æ ‡å‡†åç§°æ˜ å°„
                allowed_channels.add(variant)  # æ·»åŠ åˆ°å…è®¸çš„é¢‘é“åˆ—è¡¨
    
    # è¿‡æ»¤å¹¶é‡æ–°åˆ†ç»„é¢‘é“
    for channel in channels:
        original_name = channel['name'].strip()
        name = original_name.upper()  # è½¬æ¢ä¸ºå¤§å†™ä»¥è¿›è¡Œæ¯”è¾ƒ
        url = channel['url'].strip()
        
        # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ï¼ˆé¢‘é“å+URLï¼‰
        channel_id = f"{name}_{url}"
        
        # è·³è¿‡å·²å¤„ç†çš„é¢‘é“
        if channel_id in processed_channels:
            continue
            
        # è·å–å½“å‰é¢‘é“åç§°çš„æ‰€æœ‰å¯èƒ½å˜ä½“
        current_variants = normalize_channel_name(name)
        
        # æ£€æŸ¥é¢‘é“åæ˜¯å¦åŒ¹é…ä»»ä½•å…è®¸çš„å˜ä½“
        matched = False
        for variant in current_variants:
            if variant in allowed_channels:  # åªå¤„ç†å…è®¸çš„é¢‘é“
                standard_name = channel_name_mapping[variant].split('/')[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåç§°ä½œä¸ºæ ‡å‡†åç§°
                channel['group_title'] = f"{channel_variants[variant]}#genre#"
                channel['name'] = standard_name
                filtered_channels.append(channel)
                processed_channels.add(channel_id)
                matched = True
                break
            
    return filtered_channels


def get_group_order_from_include_list(include_list):
    """ä» include_list ä¸­è·å–åˆ†ç»„é¡ºåºå’Œæ¯ä¸ªåˆ†ç»„å†…çš„é¢‘é“é¡ºåº"""
    groups = []
    channel_order = {}  # å­˜å‚¨æ¯ä¸ªé¢‘é“çš„é¡ºåº
    current_group = None
    channel_index = 0
    
    for line in include_list:
        line = line.strip()
        if line.startswith('group:'):
            current_group = line.replace('group:', '').strip()
            if current_group not in groups:
                groups.append(current_group)
        elif line and current_group:
            # ä¿å­˜é¢‘é“çš„é¡ºåº
            channel_name = line.split('/')[0].strip()  # å–ç¬¬ä¸€ä¸ªåç§°ä½œä¸ºä¸»è¦åç§°
            channel_order[channel_name] = channel_index
            channel_index += 1
    
    return groups, channel_order


# ç”Ÿæˆ M3U æ–‡ä»¶ï¼Œå¢åŠ  EPG å›æ”¾æ”¯æŒ
def generate_m3u_file(channels, output_path, replay_days=7, custom_sort_order=None, include_list=None):
    # è·å– include_list ä¸­çš„åˆ†ç»„é¡ºåºå’Œé¢‘é“é¡ºåº
    group_order, channel_order = get_group_order_from_include_list(include_list) if include_list else ([], {})
    
    # è¯»å–éœ€è¦æµ‹é€Ÿçš„é¢‘é“åˆ—è¡¨
    test_channels = []
    try:
        with open('config/test.txt', 'r', encoding='utf-8') as f:
            test_channels = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        logging.warning("æœªæ‰¾åˆ°test.txtæ–‡ä»¶ï¼Œè·³è¿‡æµ‹é€Ÿæ’åº")
    
    test_channels_set = set(test_channels)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('#EXTM3U x-tvg-url=""\n')
        
        # æŒ‰åˆ†ç»„æ ‡é¢˜åˆ†ç»„
        group_channels = {}
        for channel in channels:
            group_title = channel['group_title'] or ''
            # æ¸…ç†åˆ†ç»„åç§°ä¸­çš„å¤šä½™å­—ç¬¦
            group_title = group_title.strip().rstrip('#genre#').rstrip(',').strip()
            if group_title not in group_channels:
                group_channels[group_title] = []
            group_channels[group_title].append(channel)

        def custom_sort_key(group_title):
            try:
                return group_order.index(group_title)
            except ValueError:
                return float('inf')

        # ä½¿ç”¨ include_list ä¸­çš„åˆ†ç»„é¡ºåº
        sorted_groups = sorted(group_channels.keys(), key=custom_sort_key)
        
        for group_title in sorted_groups:
            group = group_channels[group_title]
            
            # å¯¹åˆ†ç»„å†…çš„é¢‘é“è¿›è¡Œæ’åº
            def channel_sort_key(channel):
                channel_name = channel['name'].split('/')[0].strip()
                # é¦–å…ˆæŒ‰ç…§include_listä¸­çš„é¡ºåºæ’åº
                list_order = channel_order.get(channel_name, float('inf'))
                
                # å¦‚æœæ˜¯æµ‹é€Ÿé¢‘é“ï¼Œè¿˜è¦è€ƒè™‘é€Ÿåº¦æ’åº
                if channel_name in test_channels_set:
                    stream_time = channel.get('stream_response_time', float('inf'))
                    speed = channel.get('speed', 0)
                    # å¯¹äºç›¸åŒé¢‘é“åç§°çš„æºï¼ŒæŒ‰é€Ÿåº¦å’Œå“åº”æ—¶é—´æ’åº
                    return (list_order, -speed, stream_time)
                
                return (list_order, 0, float('inf'))
            
            sorted_group = sorted(group, key=channel_sort_key)
            
            for channel in sorted_group:
                channel_name = channel['name']
                # æ„å»ºEPGå’Œå°æ ‡ä¿¡æ¯
                tvg_id = channel_name.replace(' ', '_')
                tvg_logo = f"https://live.izbds.com/logo/{channel_name}.png"
                
                f.write(f'#EXTINF:-1 tvg-id="{tvg_id}" tvg-name="{channel_name}" tvg-logo="{tvg_logo}" group-title="{group_title}",{channel_name}\n')
                f.write(f'{channel["url"]}\n')


# ç”Ÿæˆ TXT æ–‡ä»¶
def generate_txt_file(channels, output_path, custom_sort_order=None, include_list=None):
    # è·å– include_list ä¸­çš„åˆ†ç»„é¡ºåºå’Œé¢‘é“é¡ºåº
    group_order, channel_order = get_group_order_from_include_list(include_list) if include_list else ([], {})
    
    # è¯»å–éœ€è¦æµ‹é€Ÿçš„é¢‘é“åˆ—è¡¨
    test_channels = []
    try:
        with open('config/test.txt', 'r', encoding='utf-8') as f:
            test_channels = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        logging.warning("æœªæ‰¾åˆ°test.txtæ–‡ä»¶ï¼Œè·³è¿‡æµ‹é€Ÿæ’åº")
    
    test_channels_set = set(test_channels)
    
    # æŒ‰åˆ†ç»„æ ‡é¢˜åˆ†ç»„
    group_channels = {}
    for channel in channels:
        group_title = channel['group_title'] or ''
        # æ¸…ç†åˆ†ç»„åç§°ä¸­çš„å¤šä½™å­—ç¬¦
        group_title = group_title.strip().rstrip('#genre#').rstrip(',').strip()
        if group_title not in group_channels:
            group_channels[group_title] = []
        group_channels[group_title].append(channel)

    def custom_sort_key(group_title):
        try:
            return group_order.index(group_title)
        except ValueError:
            return float('inf')

    # ä½¿ç”¨ include_list ä¸­çš„åˆ†ç»„é¡ºåº
    sorted_groups = sorted(group_channels.keys(), key=custom_sort_key)

    with open(output_path, 'w', encoding='utf-8') as f:
        # æ·»åŠ æ›´æ–°æ—¶é—´ä¿¡æ¯
        current_time = time.strftime("%Y%m%d %H:%M:%S", time.localtime())
        f.write('æ›´æ–°æ—¶é—´,#genre#\n')
        f.write(f'{current_time},https://cdn.jsdelivr.net/gh/walke2019/MYIPTV@main/output/ad/ad.mp4\n\n')
        
        for group_title in sorted_groups:
            group = group_channels[group_title]
            
            # å¯¹åˆ†ç»„å†…çš„é¢‘é“è¿›è¡Œæ’åº
            def channel_sort_key(channel):
                channel_name = channel['name'].split('/')[0].strip()
                # é¦–å…ˆæŒ‰ç…§include_listä¸­çš„é¡ºåºæ’åº
                list_order = channel_order.get(channel_name, float('inf'))
                
                # å¦‚æœæ˜¯æµ‹é€Ÿé¢‘é“ï¼Œè¿˜è¦è€ƒè™‘é€Ÿåº¦æ’åº
                if channel_name in test_channels_set:
                    stream_time = channel.get('stream_response_time', float('inf'))
                    speed = channel.get('speed', 0)
                    # å¯¹äºç›¸åŒé¢‘é“åç§°çš„æºï¼ŒæŒ‰é€Ÿåº¦å’Œå“åº”æ—¶é—´æ’åº
                    return (list_order, -speed, stream_time)
                
                return (list_order, 0, float('inf'))
            
            sorted_group = sorted(group, key=channel_sort_key)
            
            if group_title:
                f.write(f'{group_title}#genre#\n')
            for channel in sorted_group:
                f.write(f'{channel["name"]},{channel["url"]}\n')
            f.write('\n')


async def test_stream_speed(session, url, timeout=5):
    """ä½¿ç”¨aiohttpæµ‹è¯•è§†é¢‘æµé€Ÿåº¦"""
    try:
        logging.info(f"å¼€å§‹æµ‹è¯•è§†é¢‘æµ: {url}")
        start_time = time.time()
        total_size = 0
        chunk_size = 8192  # 8KB chunks
        
        async with session.get(url, timeout=timeout) as response:
            if response.status != 200:
                # å¯¹äºæŸäº›ç‰¹æ®ŠçŠ¶æ€ç ï¼Œæˆ‘ä»¬è®¤ä¸ºå¯èƒ½æ˜¯ä¸´æ—¶æ€§çš„
                if response.status in [301, 302, 307, 308]:
                    location = response.headers.get('Location')
                    if location:
                        return await test_stream_speed(session, location, timeout)
                
                logging.warning(f"è§†é¢‘æµå“åº”çŠ¶æ€ç å¼‚å¸¸: {response.status}")
                return {
                    'success': False,
                    'response_time': time.time() - start_time,
                    'error': f'HTTP status {response.status}'
                }
            
            content_type = response.headers.get('content-type', '').lower()
            
            # å¦‚æœæ˜¯m3u8æ–‡ä»¶ï¼Œè§£æå¹¶æµ‹è¯•ç¬¬ä¸€ä¸ªåˆ†ç‰‡
            if 'application/vnd.apple.mpegurl' in content_type or 'm3u8' in content_type or url.endswith('.m3u8'):
                try:
                    m3u8_content = await response.text()
                    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª.tsæ–‡ä»¶é“¾æ¥
                    ts_url = None
                    for line in m3u8_content.splitlines():
                        if line.strip() and not line.startswith('#'):
                            if line.startswith('http'):
                                ts_url = line
                            else:
                                # å¤„ç†ç›¸å¯¹è·¯å¾„
                                base_url = str(response.url)
                                if base_url.endswith('m3u8'):
                                    base_url = base_url.rsplit('/', 1)[0]
                                if not base_url.endswith('/'):
                                    base_url += '/'
                                ts_url = base_url + line
                            break
                    
                    if ts_url:
                        logging.info(f"æµ‹è¯•m3u8åˆ†ç‰‡: {ts_url}")
                        try:
                            async with session.get(ts_url, timeout=timeout) as ts_response:
                                if ts_response.status == 200:
                                    chunk_start_time = time.time()
                                    async for chunk in ts_response.content.iter_chunked(chunk_size):
                                        total_size += len(chunk)
                                        if time.time() - chunk_start_time > timeout:
                                            break
                                else:
                                    # å³ä½¿åˆ†ç‰‡è¯·æ±‚å¤±è´¥ï¼Œæˆ‘ä»¬ä¹Ÿä¸ç«‹å³åˆ¤å®šä¸ºå¤±è´¥
                                    return {
                                        'success': True,
                                        'response_time': time.time() - start_time,
                                        'speed': 0.1,
                                        'error': None
                                    }
                        except Exception as e:
                            # åˆ†ç‰‡æµ‹è¯•å¤±è´¥ï¼Œä½†m3u8å·²ç»æˆåŠŸè·å–
                            return {
                                'success': True,
                                'response_time': time.time() - start_time,
                                'speed': 0.1,
                                'error': None
                            }
                    else:
                        # m3u8è§£ææˆåŠŸä½†æ²¡æœ‰æ‰¾åˆ°åˆ†ç‰‡ï¼Œä»ç„¶è®¤ä¸ºæ˜¯å¯ç”¨çš„
                        return {
                            'success': True,
                            'response_time': time.time() - start_time,
                            'speed': 0.1,
                            'error': None
                        }
                except Exception as e:
                    logging.error(f"è§£æm3u8æ–‡ä»¶å¤±è´¥: {str(e)}")
                    # m3u8è§£æå¤±è´¥ä½†è·å–æˆåŠŸï¼Œç»™ä¸€ä¸ªè¾ƒä½çš„è¯„åˆ†
                    return {
                        'success': True,
                        'response_time': time.time() - start_time,
                        'speed': 0.05,
                        'error': f'M3U8 parse error: {str(e)}'
                    }
            else:
                # å¯¹äºém3u8æ–‡ä»¶ï¼Œç›´æ¥æµ‹è¯•æµé€Ÿåº¦
                chunk_start_time = time.time()
                async for chunk in response.content.iter_chunked(chunk_size):
                    total_size += len(chunk)
                    if time.time() - chunk_start_time > timeout:
                        break
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            speed = total_size / (1024 * 1024 * elapsed_time)  # MB/s
            
            logging.info(f"è§†é¢‘æµæµ‹è¯•å®Œæˆ - URL: {url}")
            logging.info(f"å“åº”æ—¶é—´: {elapsed_time:.2f}ç§’")
            logging.info(f"ä¸‹è½½é€Ÿåº¦: {speed:.2f} MB/s")
            
            return {
                'success': True,
                'response_time': elapsed_time,
                'speed': speed,
                'error': None
            }
            
    except asyncio.TimeoutError:
        logging.warning(f"è§†é¢‘æµæµ‹è¯•è¶…æ—¶: {url}")
        return {
            'success': False,
            'response_time': float('inf'),
            'error': 'Timeout'
        }
    except Exception as e:
        logging.error(f"è§†é¢‘æµæµ‹è¯•å¼‚å¸¸: {url}")
        logging.error(f"å¼‚å¸¸ä¿¡æ¯: {str(e)}")
        return {
            'success': False,
            'response_time': float('inf'),
            'error': str(e)
        }

async def test_specific_channels_speed(session, channels, test_channels_list):
    """æµ‹è¯•ç‰¹å®šé¢‘é“åˆ—è¡¨ä¸­çš„é¢‘é“é€Ÿåº¦"""
    test_channels_set = set(test_channels_list)
    test_results = {}
    
    total_channels = sum(1 for channel in channels if channel['name'].split('/')[0].strip() in test_channels_set)
    tested_channels = 0
    
    logging.info(f"å¼€å§‹æµ‹è¯•æŒ‡å®šé¢‘é“ï¼Œå…± {total_channels} ä¸ªé¢‘é“éœ€è¦æµ‹è¯•")
    
    # åˆ›å»ºä¿¡å·é‡æ¥é™åˆ¶å¹¶å‘æ•°
    sem = asyncio.Semaphore(10)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º10
    
    async def test_single_channel(channel):
        nonlocal tested_channels
        channel_name = channel['name'].split('/')[0].strip()
        
        async with sem:  # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            tested_channels += 1
            logging.info(f"æ­£åœ¨æµ‹è¯•ç¬¬ {tested_channels}/{total_channels} ä¸ªé¢‘é“: {channel_name}")
            
            if channel_name not in test_results:
                test_results[channel_name] = []
            
            # ä¿ç•™åŸæœ‰çš„HTTPå“åº”æ—¶é—´
            http_response_time = channel.get('response_time', float('inf'))
            logging.info(f"é¢‘é“ {channel_name} çš„HTTPå“åº”æ—¶é—´: {http_response_time:.2f}ç§’")
            
            # ä½¿ç”¨æ–°çš„æµåª’ä½“æµ‹è¯•æ–¹æ³•
            result = await test_stream_speed(session, channel['url'])
            
            # æ— è®ºæˆåŠŸä¸å¦éƒ½è®°å½•ç»“æœ
            speed = result.get('speed', 0)
            response_time = result.get('response_time', float('inf'))
            
            # å¦‚æœæµ‹è¯•å¤±è´¥ï¼Œç»™äºˆè¾ƒä½çš„è¯„åˆ†è€Œä¸æ˜¯ä¸¢å¼ƒ
            if not result['success']:
                speed = 0.01  # ç»™äºˆä¸€ä¸ªå¾ˆä½çš„é€Ÿåº¦
                response_time = float('inf')  # å“åº”æ—¶é—´è®¾ä¸ºæœ€å¤§
                
            logging.info(f"é¢‘é“ {channel_name} æµ‹è¯•å®Œæˆ")
            logging.info(f"HTTPå“åº”æ—¶é—´: {http_response_time:.2f}ç§’, è§†é¢‘æµå“åº”æ—¶é—´: {response_time:.2f}ç§’, é€Ÿåº¦: {speed:.2f} MB/s")
            
            test_results[channel_name].append({
                'url': channel['url'],
                'http_response_time': http_response_time,
                'stream_response_time': response_time,
                'speed': speed,
                'channel': channel,
                'error': result.get('error', None)
            })
    
    # åˆ›å»ºæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
    test_tasks = []
    for channel in channels:
        channel_name = channel['name'].split('/')[0].strip()
        if channel_name in test_channels_set:
            test_tasks.append(test_single_channel(channel))
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
    await asyncio.gather(*test_tasks)
    
    # å¯¹æ¯ä¸ªé¢‘é“çš„æ‰€æœ‰æºè¿›è¡Œæ’åºï¼Œä½†ä¿ç•™æ‰€æœ‰æº
    optimized_channels = []
    logging.info("\nå¼€å§‹å¤„ç†æµ‹é€Ÿç»“æœ:")
    
    for channel_name, results in test_results.items():
        if results:
            # æŒ‰é€Ÿåº¦å’Œå“åº”æ—¶é—´æ’åº
            sorted_results = sorted(results, key=lambda x: (-x.get('speed', 0), x['stream_response_time'], x['http_response_time']))
            
            # æ·»åŠ æ‰€æœ‰æºï¼Œä½†ä¿æŒæ’åº
            for result in sorted_results:
                channel = result['channel']
                channel['http_response_time'] = result['http_response_time']
                channel['stream_response_time'] = result['stream_response_time']
                channel['speed'] = result['speed']
                optimized_channels.append(channel)
                
                # åªä¸ºæœ€å¿«çš„æºæ‰“å°è¯¦ç»†æ—¥å¿—
                if result == sorted_results[0]:
                    logging.info(f"é¢‘é“: {channel_name}")
                    logging.info(f"  - æœ€ä½³æº: {channel['url']}")
                    logging.info(f"  - HTTPå“åº”æ—¶é—´: {channel['http_response_time']:.2f}ç§’")
                    logging.info(f"  - è§†é¢‘æµå“åº”æ—¶é—´: {channel['stream_response_time']:.2f}ç§’")
                    logging.info(f"  - ä¸‹è½½é€Ÿåº¦: {channel['speed']:.2f} MB/s")
    
    logging.info(f"\næµ‹é€Ÿå®Œæˆï¼Œå…±æµ‹è¯• {tested_channels} ä¸ªé¢‘é“æºï¼Œä¿ç•™æ‰€æœ‰æºä½†å·²æŒ‰é€Ÿåº¦æ’åº")
    return optimized_channels


async def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='IPTVé¢‘é“æµ‹é€Ÿå’Œæ•´ç†å·¥å…·')
    parser.add_argument('--first_test', action='store_true', help='åªæ‰§è¡Œç¬¬ä¸€æ¬¡æµ‹é€Ÿï¼ˆHTTPå“åº”æ—¶é—´æµ‹è¯•ï¼‰')
    parser.add_argument('--http_test', action='store_true', help='åªæ‰§è¡Œç¬¬äºŒæ¬¡æµ‹é€Ÿï¼ˆè§†é¢‘æµæµ‹é€Ÿï¼‰')
    args = parser.parse_args()

    # è®¾ç½®è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„
    subscribe_file = 'config/subscribe.txt'
    include_list_file = 'config/include_list.txt'
    test_channels_file = 'config/test.txt'
    
    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_dir = 'output'
    output_m3u = f'{output_dir}/result.m3u'
    output_txt = f'{output_dir}/result.txt'
    output_http_test_m3u = f'{output_dir}/result_http_test.m3u'
    output_http_test_txt = f'{output_dir}/result_http_test.txt'

    # è‡ªå®šä¹‰æ’åºé¡ºåº
    custom_sort_order = ['ğŸ„æ¹–å—é¢‘é“', 'ğŸ“å¤®è§†é¢‘é“', 'ğŸ§å«è§†é¢‘é“', 'ğŸ¦„ï¸æ¸¯Â·æ¾³Â·å°']

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # è¯»å–è®¢é˜…æ–‡ä»¶
    urls = read_subscribe_file(subscribe_file)
    if not urls:
        logging.error("è®¢é˜…æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ URLã€‚")
        return

    # è¯»å–åŒ…å«åˆ—è¡¨æ–‡ä»¶
    include_list = read_include_list_file(include_list_file)
    
    # è¯»å–éœ€è¦æµ‹é€Ÿçš„é¢‘é“åˆ—è¡¨
    test_channels = read_include_list_file(test_channels_file)

    # å¼‚æ­¥è·å–æ‰€æœ‰ URL çš„å†…å®¹
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)

    all_channels = []
    for content, _ in results:
        if content:
            if '#EXTM3U' in content:
                channels = parse_m3u_content(content)
            else:
                channels = parse_txt_content(content)
            all_channels.append(channels)

    # åˆå¹¶å¹¶å»é‡é¢‘é“
    unique_channels = merge_and_deduplicate(all_channels)

    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æµ‹é€Ÿæˆ–æ²¡æœ‰æŒ‡å®šå‚æ•°ï¼Œæ‰§è¡ŒHTTPå“åº”æ—¶é—´æµ‹è¯•
    if args.first_test or (not args.first_test and not args.http_test):
        # æµ‹è¯•æ¯ä¸ªé¢‘é“çš„å“åº”æ—¶é—´
        async with aiohttp.ClientSession() as session:
            logging.info("\n==================== ç¬¬ä¸€é˜¶æ®µï¼šHTTPå“åº”æ—¶é—´æµ‹è¯• ====================")
            tasks = [test_channel_response_time(session, channel) for channel in unique_channels]
            unique_channels = await asyncio.gather(*tasks)
            
            # ä¿å­˜ç¬¬ä¸€æ¬¡æµ‹é€Ÿç»“æœï¼ˆHTTPå“åº”æ—¶é—´æµ‹è¯•åï¼‰
            filtered_channels_first = filter_channels(unique_channels, include_list)
            generate_m3u_file(filtered_channels_first, output_m3u, custom_sort_order=custom_sort_order, include_list=include_list)
            generate_txt_file(filtered_channels_first, output_txt, custom_sort_order=custom_sort_order, include_list=include_list)
            logging.info("âœ… ç¬¬ä¸€é˜¶æ®µæµ‹è¯•å®Œæˆï¼Œå·²ä¿å­˜HTTPå“åº”æ—¶é—´æµ‹è¯•ç»“æœã€‚")
    
    # å¦‚æœæ˜¯ç¬¬äºŒæ¬¡æµ‹é€Ÿæˆ–æ²¡æœ‰æŒ‡å®šå‚æ•°ï¼Œæ‰§è¡Œè§†é¢‘æµæµ‹é€Ÿ
    if args.http_test or (not args.first_test and not args.http_test):
        # å¯¹ç‰¹å®šé¢‘é“è¿›è¡Œæµ‹é€Ÿ
        if test_channels:
            async with aiohttp.ClientSession() as session:
                logging.info("\n==================== ç¬¬äºŒé˜¶æ®µï¼šè§†é¢‘æµæµ‹é€Ÿ ====================")
                logging.info(f"å³å°†æµ‹è¯•ä»¥ä¸‹é¢‘é“çš„è§†é¢‘æµè´¨é‡ï¼š{', '.join(test_channels)}")
                optimized_channels = await test_specific_channels_speed(session, unique_channels, test_channels)
                
                # æ›´æ–°åŸå§‹é¢‘é“åˆ—è¡¨ä¸­çš„å“åº”æ—¶é—´
                optimized_channels_dict = {f"{ch['name']}_{ch['url']}": ch for ch in optimized_channels}
                for channel in unique_channels:
                    channel_key = f"{channel['name']}_{channel['url']}"
                    if channel_key in optimized_channels_dict:
                        channel.update(optimized_channels_dict[channel_key])
                
                # è¿‡æ»¤é¢‘é“
                filtered_channels = filter_channels(unique_channels, include_list)
                
                # ç”Ÿæˆæœ€ç»ˆçš„ M3U å’Œ TXT æ–‡ä»¶
                logging.info("\nç”Ÿæˆæœ€ç»ˆæ–‡ä»¶ï¼ˆåŒ…å«æµ‹é€Ÿç»“æœï¼‰...")
                generate_m3u_file(filtered_channels, output_http_test_m3u, custom_sort_order=custom_sort_order, include_list=include_list)
                generate_txt_file(filtered_channels, output_http_test_txt, custom_sort_order=custom_sort_order, include_list=include_list)
                logging.info("âœ… ç¬¬äºŒé˜¶æ®µæµ‹è¯•å®Œæˆï¼Œå·²æ›´æ–°é¢‘é“æµ‹é€Ÿä¿¡æ¯ã€‚")
        else:
            logging.warning("âš ï¸ æœªæ‰¾åˆ°éœ€è¦æµ‹é€Ÿçš„é¢‘é“åˆ—è¡¨ï¼Œè·³è¿‡ç¬¬äºŒé˜¶æ®µæµ‹é€Ÿã€‚")
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå…·ä½“æµ‹è¯•ï¼Œåˆ™æ‰§è¡Œå®Œæ•´æµç¨‹
    if not args.first_test and not args.http_test:
        logging.info("\n==================== æµ‹é€Ÿä»»åŠ¡å®Œæˆ ====================")
        logging.info("âœ… å·²ç”Ÿæˆæ‰€æœ‰ç»“æœæ–‡ä»¶ï¼š")
        logging.info(f"  - {output_m3u}ï¼šç¬¬ä¸€é˜¶æ®µHTTPæµ‹é€Ÿç»“æœ")
        logging.info(f"  - {output_txt}ï¼šç¬¬ä¸€é˜¶æ®µHTTPæµ‹é€Ÿç»“æœï¼ˆTXTæ ¼å¼ï¼‰")
        logging.info(f"  - {output_http_test_m3u}ï¼šç¬¬äºŒé˜¶æ®µè§†é¢‘æµæµ‹é€Ÿç»“æœ")
        logging.info(f"  - {output_http_test_txt}ï¼šç¬¬äºŒé˜¶æ®µè§†é¢‘æµæµ‹é€Ÿç»“æœï¼ˆTXTæ ¼å¼ï¼‰")
    elif args.first_test:
        logging.info("\n==================== ç¬¬ä¸€é˜¶æ®µæµ‹é€Ÿä»»åŠ¡å®Œæˆ ====================")
        logging.info("âœ… å·²ç”Ÿæˆç¬¬ä¸€é˜¶æ®µæµ‹é€Ÿç»“æœæ–‡ä»¶ï¼š")
        logging.info(f"  - {output_m3u}ï¼šHTTPå“åº”æ—¶é—´æµ‹è¯•ç»“æœ")
        logging.info(f"  - {output_txt}ï¼šHTTPå“åº”æ—¶é—´æµ‹è¯•ç»“æœï¼ˆTXTæ ¼å¼ï¼‰")
    elif args.http_test:
        logging.info("\n==================== ç¬¬äºŒé˜¶æ®µæµ‹é€Ÿä»»åŠ¡å®Œæˆ ====================")
        logging.info("âœ… å·²ç”Ÿæˆç¬¬äºŒé˜¶æ®µæµ‹é€Ÿç»“æœæ–‡ä»¶ï¼š")
        logging.info(f"  - {output_http_test_m3u}ï¼šè§†é¢‘æµæµ‹é€Ÿç»“æœ")
        logging.info(f"  - {output_http_test_txt}ï¼šè§†é¢‘æµæµ‹é€Ÿç»“æœï¼ˆTXTæ ¼å¼ï¼‰")


if __name__ == '__main__':
    asyncio.run(main())
